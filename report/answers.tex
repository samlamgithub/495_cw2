\documentclass[12pt,twoside]{article}

\newcommand{\reporttitle}{495 - Advanced Statistical Machine Learning and Pattern Recognition Coursework 2}
\newcommand{\reportauthor}{Jiahao Lin}
\newcommand{\reporttype}{Coursework 2}
\newcommand{\cid}{00837321}

% include files that load packages and define macros
\input{includes} % various packages needed for maths etc.
\input{notation} % short-hand notation and macros


%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% front page
\input{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Main document
\section{Exercise I}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\subsection{}
fdss
\subsection{}
\begin{enumerate}[(ii)]
\item
fsds
\end{enumerate}
\section{Exercise II}
\begin{enumerate}[(i)]
\item

N sequences, T observations per sequence, 5 states\\
Estimate $\theta = \left\lbrace\vec{\pi}, A\right\rbrace$\\
$\vec{\pi} = \left\lbrace \pi_1 ... \pi_5\right\rbrace$\\
for $ l = 1..N : D_l = \left\lbrace x_1^l ...  x_T^l \right\rbrace$\\
$\vec{A} = [a_{ij}]$\\
\begin{align}
\vec{A} = \matdet{
0 & a_{12} & 0 & 0 & 0\\
0 & a_{22} & a_{22} & 0 & 0\\
0 & 0 & a_{33} & a_{34} & a_{34}\\
0 & 0 & 0 & a_{44} & a_{45}\\
0 & 0 & 0 & 0 & a_{55}\\
} \\
x_t^l = \left\lbrace\colvec{1,0,0,0,0},\, \colvec{0,1,0,0,0},\, \colvec{0,0,1,0,0}, \colvec{0,0,0,1,0} \colvec{0,0,0,0,1} \right\rbrace
\end{align}
Maximise $p(D_1 ... D_N | \theta)$\\
\begin{align}
p(D_1 ... D_N | \theta) \\
= \prod_{l=1}^{N} p(D_l|\theta) 
\end{align}
Because
\begin{align}
p(D_1 | \theta) \\
= p(x_1^l ... x_T^l | \theta) \\
= p(x_1^l) \prod_{t=2}^{T} p(x_t^l |x_{t-1}^l) \\
= \prod_{k=1}^5 \pi_k^{x_{1k}^l} \prod_{T=2}^T \prod_{j=1}^5 \prod_{k=1}^5 a_{jk}^{x_{t-1j}^l x_{tk}^l}
\end{align}
so
\begin{align}
p(D_1 ... D_N | \theta) \\
= \prod_{l=1}^{N} \prod_{k=1}^5 \pi_k^{x_{1k}^l} \prod_{T=2}^T \prod_{j=1}^5 \prod_{k=1}^5 a_{jk}^{x_{t-1j}^l x_{tk}^l}
\end{align}
Take the log
\begin{align}
\ln(p(D_1 ... D_N | \theta)) \\
= \sum_{l=1}^{N} \sum_{k=1}^5 x_{1k}^l \ln(\pi_k) +  \sum_{l=1}^{N} \sum_{T=2}^T \sum_{j=1}^5 \sum_{k=1}^5 x_{t-1j}^l x_{tk}^l \ln(a_{jk})
\end{align}
Now we take
\begin{align}
N_k^l = \sum_{l=1}^N x_{1k}^l \\
N_{jk} = \sum_{l=1}^N \sum_{T=2}^T x_{t-1j}^l x_{tk}^l  
\end{align}
so
\begin{align}
\ln(p(D_1 ... D_N | \theta)) \\
= \sum_{l=1}^{N}N_k^l \ln (\pi_k) +  \sum_{j=1}^5 \sum_{k=1}^5  N_{jk} \ln( a_{jk} )
\end{align}
When maximising $\ln(p(D_1 ... D_N | \theta)) $, we put constraints:
\begin{align}
\sum_{k=1}^{K}\pi_k = 1\\
\sum_{k=1}^{K} a_{jk} = 1
\end{align}
So
\begin{align}
\vec{L}(\vec{\pi}, \vec{A}) = \sum_{l=1}^{N}N_k^l \ln (\pi_k) +  \sum_{j=1}^5 \sum_{k=1}^5  N_{jk} \ln (a_{jk}) - \lambda (\sum_{k=1}^{K}\pi_k - 1) - \gamma (\sum_{k=1}^{K} a_{jk} -1 )
\end{align}
Maximise $\vec{L}(\vec{\pi}, \vec{A})$:
\begin{align}
 \frac{\partial \vec{L}(\vec{\pi}, \vec{A})}{\partial \pi_k} = \frac{N_k^l}{\pi_k} - \lambda  \\
\frac{\partial  \vec{L}(\vec{\pi}, \vec{A})}{\partial \pi_k} = 0 \\
\lambda = \frac{N_k^l}{\pi_k} \\
\sum_{k=1}^{K}\pi_k = 1 = \sum_{k=1}^{K} \frac{N_k^l}{\lambda}\\
\lambda = \sum_{k=1}^{K} N_k^l\\
\pi_k = \frac{N_k^l}{\sum_{k=1}^{K} N_k^l} \\
\frac{\partial \vec{L}(\vec{\pi}, \vec{A})}{\partial a_{jk}} = \frac{N_{jk} }{a_{jk}} - \gamma \\
\gamma =  \sum_{k=1}^{K} N_{jk} \\
a_{jk} = \frac{N_{jk}}{\sum_{k=1}^{K} N_{jk}} 
\end{align}
Given our matrix $\vec{A}$, we know that:
\begin{align}
a_{12} = 1 \\
a_{22} = \frac{N_{22}}{\sum_{k=1}^{K} N_{2k}} \\
a_{23} = 1-a_{22} = \frac{N_{22}}{\sum_{k=1}^{K} N_{2k}} \\
a_{33} =  \frac{N_{33}}{\sum_{k=1}^{K} N_{3k}} \\
a_{34} =  \frac{N_{34}}{\sum_{k=1}^{K} N_{3k}} \\
a_{35} = 1- a_{34} - a_{33} = 1- \frac{N_{33} + N_{34}}{\sum_{k=1}^{K} N_{3k}} \\
a_{44} =  \frac{N_{44}}{\sum_{k=1}^{K} N_{4k}} \\
a_{45} = 1- a_{44} = 1-  \frac{N_{44}}{\sum_{k=1}^{K} N_{4k}} \\
a_{55} = 1 \\
\pi_k = \frac{N_k^l}{\sum_{k=1}^{K} N_k^l} 
\end{align}


\item
K latent states, N sequences, T observations per sequence, 5 Observation state\\
\begin{align}
p(z_1^l | \pi) = \prod_{c=1}^K \pi_k^{z_{1c}^l} \\
p(z_t^l|z_{t-1}^l, A) = \prod_{j=1}^K \prod_{k=1}^K a_{jk}^{z_{t-1j}^l z_{tk}^l} \\
p(z_{1k}^l = 1) = \pi_k \\
\vec{\pi} = \left\lbrace \pi_1 ... \pi_5\right\rbrace \\
\vec{A} = [a_{ij}] \\
p(x_t^l| z_t^l) = \prod_{j=1}^5 \prod_{k=1}^K b_{kj} ^ {x_{t-1j}^l x_{tk}^l} \\
Estimate\quad \theta = \left\lbrace\vec{\pi}, A,\vec{B} \right\rbrace\\
for\quad l = 1..N : D_l = \left\lbrace x_1^l ...  x_T^l \right\rbrace 
\end{align}
Maximise $p(D_1 ... D_N, Z_1 ... Z_N | \theta)$\\
\begin{align}
x_t^l = \left\lbrace\colvec{1,0,0,0,0},\, \colvec{0,1,0,0,0},\, \colvec{0,0,1,0,0}, \colvec{0,0,0,1,0} \colvec{0,0,0,0,1} \right\rbrace \\
p(D_1 ... D_N, Z_1 ... Z_N | \theta) = \prod_{l=1}^{N}  p(x_1^l ... x_T^l, z_1^l ... z_T^l | \theta) \\
= \prod_{l=1}^{N} \prod_{t=1}^{T}  p(x_t^l |z_t^l) p(z_1^l) \prod_{t=2}^{T} p(z_t^l |z_{t-1}^l) \\
= \prod_{l=1}^{N} \prod_{t=1}^{T} \prod_{j=1}^5 \prod_{k=1}^K b_{kj} ^ {x_{tj}^l z_{tk}^l}    \prod_{k=1}^K \pi_k^{z_{1k}^l} \prod_{t=2}^{T} \prod_{j=1}^K \prod_{k=1}^K a_{jk}^{z_{t-1j}^l z_{tk}^l}
\end{align}
Take the log of this posterior
\begin{align}
\ln (p(D_1 ... D_N ,  Z_1 ... Z_N | \theta)) \\
= \sum_{l=1}^{N} \sum_{t=1}^{T} \sum_{j=1}^5 \sum_{k=1}^K x_{tj}^l z_{tk}^l \ln (b_{kj})  +\sum_{l=1}^{N} \sum_{k=1}^K z_{1k}^l \ln (\pi_k) + \sum_{l=1}^{N} \sum_{t=2}^{T} \sum_{j=1}^K \sum_{k=1}^K z_{t-1j}^l z_{tk}^l \ln (a_{jk}) 
\end{align}
Take expectation with respect to posterior
\begin{align}
E_{[z_{tk}^l]} [\ln (p(D, Z|\theta))] = \sum_{l=1}^{N} \sum_{t=1}^{T} \sum_{j=1}^5 \sum_{k=1}^K x_{tj}^l E[z_{tk}^l] \ln (b_{kj}) \\  +\sum_{l=1}^{N} \sum_{k=1}^K E[z_{1k}^l] \ln (\pi_k) + \sum_{l=1}^{N} \sum_{t=2}^{T} \sum_{j=1}^K \sum_{k=1}^K E[z_{t-1j}^l  z_{tk}^l] \ln (a_{jk})
\end{align}
To maximise it we need to put constraints
\begin{align}
\sum_{k=1}^{5} b_{jk} = 1\\
\sum_{k=1}^{5} \pi_k = 1\\
\sum_{k=1}^{K} a_{jk} = 1
\end{align}
So
\begin{align}
\vec{L}(\vec{\pi}, \vec{A} , \vec{B}) =  \sum_{l=1}^{N} \sum_{t=1}^{T} \sum_{j=1}^5 \sum_{k=1}^K x_{tj}^l E[z_{tk}^l] \ln (b_{kj})  + \sum_{l=1}^{N} \sum_{k=1}^K E[z_{1k}^l] \ln (\pi_k) \\
+ \sum_{l=1}^{N} \sum_{t=2}^{T} \sum_{j=1}^K \sum_{k=1}^K E[z_{t-1j}^l  z_{tk}^l] \ln (a_{jk}) - \lambda (  \sum_{k=1}^{5} b_{jk} -1 ) - \gamma (\sum_{k=1}^{5} \pi_k - 1) - \mu (\sum_{k=1}^{K} a_{jk} - 1)
\end{align}
Maximise $\vec{L}(\vec{\pi},\vec{A},\vec{B})$:
\begin{align}
\frac{\partial \vec{L}(\vec{\pi}, \vec{A}, \vec{B})}{\partial  b_{jk}} = \frac{\sum_{l=1}^{N} \sum_{t=1}^{T} E[z_{tk}^l]}{b_{jk}} - \lambda = 0 \\
\lambda = \frac{\sum_{l=1}^{N} \sum_{t=1}^{T} E[z_{tk}^l]}{b_{jk}} \\
b_{jk} = \frac{\sum_{l=1}^{N} \sum_{t=1}^{T} x_{tj}^l E[z_{tk}^l]}{\sum_{l=1}^{N} \sum_{t=1}^{T} \sum_{j=1}^K E[z_{tk}^l]}\\
\frac{\partial \vec{L}(\vec{\pi}, \vec{A}, \vec{B})}{\partial  \pi_k} = \frac{\sum_{l=1}^{N} \sum_{k=1}^K E[z_{1k}^l] }{\pi_k}- \gamma = 0\\ 
\gamma = \frac{\sum_{l=1}^{N} \sum_{k=1}^K E[z_{1k}^l] }{\pi_k} \\
\pi_k = \frac{\sum_{l=1}^{N} E[z_{1k}^l]}{\sum_{l=1}^{N} \sum_{r=1}^K E[z_{1r}^l]} \\
\frac{\partial \vec{L}(\vec{\pi}, \vec{A}, \vec{B})}{\partial  a_{jk}} =  \frac{\sum_{l=1}^{N} \sum_{t=2}^{T} E[z_{t-1j}^l  z_{tk}^l]}{a_{jk}}- \mu = 0\\
a_{jk} = \frac{\sum_{l=1}^{N} \sum_{t=2}^{T} E[z_{t-1j}^l  z_{tk}^l]}{\sum_{l=1}^{N} \sum_{t=2}^{T} \sum_{r=1}^K E[z_{t-1j}^l  z_{tr}^l]} 
\end{align}
Answer is
\begin{align}
b_{jk} = \frac{\sum_{l=1}^{N} \sum_{t=1}^{T} x_{tj}^l E[z_{tk}^l]}{\sum_{l=1}^{N} \sum_{t=1}^{T} \sum_{j=1}^K E[z_{tk}^l]}\\
\pi_k = \frac{\sum_{l=1}^{N} E[z_{1k}^l]}{\sum_{l=1}^{N} \sum_{r=1}^K E[z_{1r}^l]} \\
a_{jk} = \frac{\sum_{l=1}^{N} \sum_{t=2}^{T} E[z_{t-1j}^l  z_{tk}^l]}{\sum_{l=1}^{N} \sum_{t=2}^{T} \sum_{r=1}^K E[z_{t-1j}^l  z_{tr}^l]}
\end{align}

\end{enumerate}










\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
